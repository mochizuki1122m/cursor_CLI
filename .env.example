## LLM connection
LLM_PROVIDER=cursor
CURSOR_API_BASE=
CURSOR_API_KEY=
OPENAI_API_BASE=
OPENAI_API_KEY=
LITELLM_PROXY_URL=
LITELLM_API_KEY=

## Model defaults
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.15
LLM_TOP_P=0.8
LLM_USE_PRESET=true
LLM_TIMEOUT_MS=45000
LLM_MAX_TOKENS=1000
LLM_MAX_RETRIES=3
LLM_BACKOFF_MS=600
LLM_CB_FAILURE_THRESHOLD=3
LLM_CB_OPEN_MS=300000

## Prompts directory
PROMPTS_DIR=prompts

## Relay limits
MAX_ROUNDS=6
MAX_DIFF_LOC=400
MAX_CHANGED_FILES=10
MAX_API_TOKENS_PER_ROUND=150000
LLM_PROVIDER=http
OPENAI_API_BASE=https://api.openai.com
OPENAI_API_KEY=YOUR_API_KEY_HERE
# Or for Cursor-compatible endpoint:
# CURSOR_API_BASE=
# CURSOR_API_KEY=
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.15
LLM_TOP_P=0.8
LLM_USE_PRESET=true
# PROMPTS_DIR=prompts
# LLM_TIMEOUT_MS=45000
# LLM_MAX_TOKENS=1000
# LLM_MAX_RETRIES=3
# LLM_BACKOFF_MS=600
# LLM_CB_FAILURE_THRESHOLD=3
# LLM_CB_OPEN_MS=300000
# MAX_ROUNDS=6
# MAX_DIFF_LOC=400
# MAX_CHANGED_FILES=10
# MAX_API_TOKENS_PER_ROUND=150000
LLM_PROVIDER=cursor # http | cursor
CURSOR_API_BASE=http://localhost:11434/v1
CURSOR_API_KEY=your-cursor-key

LITELLM_PROXY_URL=http://your-gateway:8000
LITELLM_API_KEY=your-gateway-access-token
OPENAI_API_BASE=${LITELLM_PROXY_URL}
OPENAI_API_KEY=${LITELLM_API_KEY}
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.15
LLM_TOP_P=0.8
MAX_ROUNDS=6
MAX_DIFF_LOC=400
MAX_CHANGED_FILES=10
MAX_API_TOKENS_PER_ROUND=150000
